{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a52b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "# Load data from Phase 4\n",
    "X_train = pd.read_csv('data/X_train_scaled.csv')\n",
    "X_test = pd.read_csv('data/X_test_scaled.csv')\n",
    "y_train = pd.read_csv('data/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('data/y_test.csv').values.ravel()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"✓ PHASE 6: HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hyperparameters to try\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "print(\"\\n1. Running Grid Search (this takes 5-10 minutes)...\")\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid = GridSearchCV(rf, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n✓ Best parameters:\")\n",
    "for param, value in grid.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(f\"\\nBest CV F1-Score: {grid.best_score_:.4f}\")\n",
    "\n",
    "# Test set evaluation\n",
    "print(f\"\\n2. Evaluating on test set...\")\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"   Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   Test F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Detailed report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save tuned model\n",
    "with open('models/best_model_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"\\n✓ Tuned model saved: models/best_model_tuned.pkl\")\n",
    "print(f\"✓ PHASE 6 COMPLETE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative tuning method using RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define base estimator (use RandomForest as in grid search)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Larger parameter space for randomized search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, param_dist, n_iter=20, cv=5, scoring='f1', n_jobs=-1, random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"RandomizedSearch best params: {random_search.best_params_}\")\n",
    "print(f\"RandomizedSearch best CV F1: {random_search.best_score_:.4f}\")\n",
    "\n",
    "best_random_model = random_search.best_estimator_\n",
    "# Ensure models directory exists and save\n",
    "os.makedirs('models', exist_ok=True)\n",
    "with open('models/best_model_randomized.pkl', 'wb') as f:\n",
    "    pickle.dump(best_random_model, f)\n",
    "print(\"Saved tuned randomized model to models/best_model_randomized.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a147b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sophisticated Bayesian tuning (optional)\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "except Exception:\n",
    "    BayesSearchCV = None\n",
    "    print(\"Optional package 'scikit-optimize' (skopt) is not installed.\")\n",
    "    print(\"To enable Bayesian search, install it with:\")\n",
    "    print(\"  pip install scikit-optimize\")\n",
    "    print(\"Or use RandomizedSearchCV / GridSearchCV (already included above) as alternatives.\")\n",
    "\n",
    "# If BayesSearchCV is available, you can use it like this (example):\n",
    "if BayesSearchCV is not None:\n",
    "    # Example parameter search space (requires skopt)\n",
    "    from skopt.space import Integer, Real, Categorical\n",
    "    bayes_space = {\n",
    "        'n_estimators': Integer(50, 500),\n",
    "        'max_depth': Integer(5, 50),\n",
    "        'min_samples_split': Integer(2, 20),\n",
    "        'min_samples_leaf': Integer(1, 10),\n",
    "    }\n",
    "    # Example usage (uncomment to run):\n",
    "    # bayes_search = BayesSearchCV(rf, bayes_space, n_iter=30, cv=5, scoring='f1', n_jobs=-1, random_state=42)\n",
    "    # bayes_search.fit(X_train, y_train)\n",
    "    # print(bayes_search.best_params_)\n",
    "else:\n",
    "    # Skipping Bayesian search because skopt is not available\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d2786",
   "metadata": {},
   "source": [
    "## Phase 6 — Hyperparameter Tuning\n",
    "\n",
    "This document summarizes hyperparameter tuning performed in `Notebooks/Phase6.ipynb`, lists the artifacts produced, and provides quick run and troubleshooting instructions.\n",
    "\n",
    "- **Purpose:** Improve model performance by searching for better hyperparameters using Grid Search, Randomized Search, and (optionally) Bayesian Search.\n",
    "- **Notebook:** `Notebooks/Phase6.ipynb`\n",
    "\n",
    "**Produced Artifacts**\n",
    "- `models/best_model_tuned.pkl`: Best estimator found by `GridSearchCV` (saved after grid search).\n",
    "- `models/best_model_randomized.pkl`: Best estimator found by `RandomizedSearchCV` (saved after randomized search).\n",
    "- `models/best_model_randomized.pkl` and `models/best_model_tuned.pkl` can be used for evaluation or deployment.\n",
    "\n",
    "**Main Steps (high level)**\n",
    "- Load preprocessed training and test data from Phase 4 (`data/X_train_scaled.csv`, `data/X_test_scaled.csv`, `data/y_train.csv`, `data/y_test.csv`).\n",
    "- Run `GridSearchCV` on a `RandomForestClassifier` using a predefined `param_grid` and `scoring='f1'` to find a tuned model.\n",
    "- Evaluate the best grid-search estimator on the test set and print accuracy/F1 and a classification report.\n",
    "- Optionally run `RandomizedSearchCV` for a larger parameter space (faster than exhaustive grid search) and save the result.\n",
    "- An optional Bayesian optimization example (using `scikit-optimize` / `skopt`) is provided but guarded by a try/except: install `scikit-optimize` to enable it.\n",
    "\n",
    "**How to run (PowerShell)**\n",
    "1. From the project root, execute the notebook headless (example):\n",
    "\n",
    "```powershell\n",
    "python -m nbconvert --to notebook --execute \"Notebooks\\Phase6.ipynb\" --output \"Notebooks\\Phase6_executed.ipynb\"\n",
    "```\n",
    "\n",
    "2. Or run interactively in VS Code / Jupyter and execute cells in order.\n",
    "\n",
    "**Notes & Troubleshooting**\n",
    "- Missing `data/` files: Phase 6 depends on outputs from Phase 4. Ensure `data/X_train_scaled.csv`, `data/X_test_scaled.csv`, `data/y_train.csv`, and `data/y_test.csv` exist.\n",
    "- Long runtime: Grid search (cv=5) over many parameters can be slow. Use `RandomizedSearchCV` with `n_iter` set to a reasonable value (e.g., 20) to reduce runtime.\n",
    "- `ModuleNotFoundError: No module named 'skopt'`: The notebook includes an optional `BayesSearchCV` example requiring the `scikit-optimize` package. Install it with:\n",
    "\n",
    "```powershell\n",
    "pip install scikit-optimize\n",
    "# or with conda\n",
    "conda install -c conda-forge scikit-optimize\n",
    "```\n",
    "\n",
    "If you don't want to install additional packages, use `GridSearchCV` or `RandomizedSearchCV` (both included in the notebook).\n",
    "\n",
    "- Memory/CPU limits: Use `n_jobs` carefully (e.g., `n_jobs=-1` uses all CPUs). On constrained machines, set `n_jobs=1` or reduce `n_estimators`.\n",
    "\n",
    "- Reproducibility: Randomized and Bayesian searches accept `random_state` for reproducible results.\n",
    "\n",
    "**Example parameters used in the notebook**\n",
    "- Grid search `param_grid` (example):\n",
    "  - `n_estimators`: [100, 200, 300]\n",
    "  - `max_depth`: [10, 20, 30, None]\n",
    "  - `min_samples_split`: [2, 5, 10]\n",
    "  - `min_samples_leaf`: [1, 2, 4]\n",
    "\n",
    "- Randomized search `param_dist` (example):\n",
    "  - `n_estimators`: [100, 200, 300, 400, 500]\n",
    "  - `max_depth`: [10, 20, 30, None]\n",
    "  - `min_samples_split`: [2, 5, 10]\n",
    "  - `min_samples_leaf`: [1, 2, 4]\n",
    "  - `bootstrap`: [True, False]\n",
    "\n",
    "**Next steps**\n",
    "- Run Phase 6 and verify `models/best_model_tuned.pkl` and/or `models/best_model_randomized.pkl` are created.\n",
    "- Optionally perform further tuning using Bayesian optimization (`skopt`) or Optuna for more advanced search strategies.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
