{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ed077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, roc_curve, confusion_matrix, \n",
    "                             classification_report)\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "X_test = pd.read_csv('data/X_test_scaled.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv').values.ravel()\n",
    "\n",
    "# Load tuned model\n",
    "with open('models/best_model_tuned.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ PHASE 7: MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(f\"\\n1. PERFORMANCE METRICS\")\n",
    "print(f\"   Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"   Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"   Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"   F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"   ROC-AUC:   {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\n2. CONFUSION MATRIX\")\n",
    "print(f\"   TN: {cm[0,0]}  FP: {cm[0,1]}\")\n",
    "print(f\"   FN: {cm[1,0]}  TP: {cm[1,1]}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\n3. CLASSIFICATION REPORT\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve (left subplot)\n",
    "axes[0].plot(fpr, tpr, label=f'ROC (AUC={auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC-AUC Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Confusion Matrix (right subplot)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "plt.savefig('visualizations/14_evaluation.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "try:\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_test.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(f\"\\n4. TOP 10 FEATURES\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "    # Visualize top features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barh(importance_df['Feature'][:15], importance_df['Importance'][:15])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 15 Most Important Features')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig('visualizations/15_feature_importance.png', dpi=300)\n",
    "    plt.show()\n",
    "except AttributeError:\n",
    "    print('\\nModel does not expose feature_importances_. Skipping feature importance.')\n",
    "\n",
    "print(f\"\\nâœ“ PHASE 7 COMPLETE!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02bca0",
   "metadata": {},
   "source": [
    "## ðŸ“‹ EVALUATION METRICS REFERENCE\n",
    "\n",
    "| Metric | Formula | Meaning |\n",
    "|--------|---------|---------|\n",
    "| **Accuracy** | (TP+TN)/Total | % of correct predictions overall |\n",
    "| **Precision** | TP/(TP+FP) | % of predicted disease cases that are correct |\n",
    "| **Recall** | TP/(TP+FN) | % of actual disease cases that are detected |\n",
    "| **F1-Score** | 2(PÃ—R)/(P+R) | Harmonic mean balancing precision & recall |\n",
    "| **ROC-AUC** | Area under curve | Model's discrimination ability (0.5=random, 1.0=perfect) |\n",
    "\n",
    "**Key Definitions:**\n",
    "- **TP (True Positive):** Correctly predicted disease\n",
    "- **TN (True Negative):** Correctly predicted no disease\n",
    "- **FP (False Positive):** Incorrectly predicted disease (Type I error)\n",
    "- **FN (False Negative):** Incorrectly predicted no disease (Type II error)\n",
    "\n",
    "**Goal: All > 0.90 (90%)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d004041",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ CONFUSION MATRIX GUIDE\n",
    "\n",
    "| | **Predicted No** | **Predicted Yes** |\n",
    "|---|---|---|\n",
    "| **Actual No** | TN (Correct) âœ“ | FP (Wrong) âœ— |\n",
    "| **Actual Yes** | FN (Wrong) âœ— | TP (Correct) âœ“ |\n",
    "\n",
    "**Interpretation:**\n",
    "- **TN (True Negative):** Model correctly predicts no disease when patient is healthy\n",
    "- **TP (True Positive):** Model correctly predicts disease when patient has disease\n",
    "- **FP (False Positive):** Model incorrectly predicts disease (Type I error) â€” patient is healthy but flagged as diseased\n",
    "- **FN (False Negative):** Model incorrectly predicts no disease (Type II error) â€” patient has disease but not detected\n",
    "\n",
    "**Clinical Impact:**\n",
    "- High FP: Unnecessary patient stress and follow-up tests\n",
    "- High FN: **Dangerous** â€” missed disease diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one test patient\n",
    "new_patient = X_test.iloc[0:1]\n",
    "\n",
    "# Predict\n",
    "disease_prob = model.predict_proba(new_patient)[0, 1]\n",
    "prediction = model.predict(new_patient)\n",
    "\n",
    "# Output\n",
    "print(f\"Disease Probability: {disease_prob:.2%}\")\n",
    "print(f\"Risk Level: {'HIGH' if disease_prob > 0.7 else 'MODERATE' if disease_prob > 0.3 else 'LOW'}\")\n",
    "print(f\"Prediction: {'Has Disease' if prediction else 'No Disease'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1fa41",
   "metadata": {},
   "source": [
    "## Phase 7 â€” Model Evaluation & Results\n",
    "\n",
    "This document summarizes the model evaluation and interpretation performed in `Notebooks/Phase7.ipynb`, lists the artifacts produced, and provides quick run and troubleshooting instructions.\n",
    "\n",
    "- **Purpose:** Evaluate the tuned model on the test set using comprehensive metrics and visualizations. Generate evaluation reports, confusion matrices, ROC curves, and feature importance analysis.\n",
    "- **Notebook:** `Notebooks/Phase7.ipynb`\n",
    "\n",
    "**Produced Artifacts**\n",
    "- `visualizations/14_evaluation.png`: Side-by-side ROC curve and confusion matrix heatmap.\n",
    "- `visualizations/15_feature_importance.png`: Bar chart of top 15 most important features (if model supports it).\n",
    "- Console output: Performance metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC), confusion matrix, classification report, and top 10 features table.\n",
    "\n",
    "**Main Steps (high level)**\n",
    "- Load preprocessed test data and the tuned model from Phase 6 (`models/best_model_tuned.pkl`).\n",
    "- Generate predictions on the test set.\n",
    "- Calculate key metrics:\n",
    "  - **Accuracy:** Overall correctness of predictions.\n",
    "  - **Precision:** % of predicted disease cases that are correct.\n",
    "  - **Recall:** % of actual disease cases detected.\n",
    "  - **F1-Score:** Harmonic mean of precision and recall.\n",
    "  - **ROC-AUC:** Model's discrimination ability across all thresholds.\n",
    "- Generate confusion matrix and classification report.\n",
    "- Visualize ROC curve and confusion matrix side-by-side.\n",
    "- Extract and visualize top feature importances (if available).\n",
    "\n",
    "**How to run (PowerShell)**\n",
    "1. From the project root, execute the notebook headless (example):\n",
    "\n",
    "```powershell\n",
    "python -m nbconvert --to notebook --execute \"Notebooks\\Phase7.ipynb\" --output \"Notebooks\\Phase7_executed.ipynb\"\n",
    "```\n",
    "\n",
    "2. Or run interactively in VS Code / Jupyter and execute cells in order.\n",
    "\n",
    "**Metrics Reference**\n",
    "\n",
    "| Metric | Formula | Meaning |\n",
    "|--------|---------|---------|\n",
    "| **Accuracy** | (TP+TN)/Total | % of correct predictions overall |\n",
    "| **Precision** | TP/(TP+FP) | % of predicted disease cases that are correct |\n",
    "| **Recall** | TP/(TP+FN) | % of actual disease cases that are detected |\n",
    "| **F1-Score** | 2(PÃ—R)/(P+R) | Harmonic mean balancing precision & recall |\n",
    "| **ROC-AUC** | Area under curve | Model's discrimination ability (0.5=random, 1.0=perfect) |\n",
    "\n",
    "**Confusion Matrix Guide**\n",
    "\n",
    "| | **Predicted No** | **Predicted Yes** |\n",
    "|---|---|---|\n",
    "| **Actual No** | TN (Correct) âœ“ | FP (Wrong) âœ— |\n",
    "| **Actual Yes** | FN (Wrong) âœ— | TP (Correct) âœ“ |\n",
    "\n",
    "- **TN:** Model correctly predicts no disease when patient is healthy.\n",
    "- **TP:** Model correctly predicts disease when patient has disease.\n",
    "- **FP:** Model incorrectly predicts disease (Type I error).\n",
    "- **FN:** Model incorrectly predicts no disease (Type II error) â€” **Dangerous in medical context**.\n",
    "\n",
    "**Notes & Troubleshooting**\n",
    "- Missing `models/best_model_tuned.pkl`: Phase 7 loads the tuned model from Phase 6. Ensure Phase 6 has been executed successfully.\n",
    "- Missing test data: Phase 7 requires `data/X_test_scaled.csv`, `data/X_test_scaled.csv`, `data/y_test.csv`, and `data/y_test.csv` (created in Phase 4). Run Phase 4 first if these files are absent.\n",
    "- `FileNotFoundError` when saving visualizations: The notebook creates the `visualizations/` directory automatically via `os.makedirs('visualizations', exist_ok=True)` before saving. If this fails, check file system permissions.\n",
    "- Model doesn't have `feature_importances_`: Some models (e.g., SVM, KNN) don't expose feature importances. The notebook gracefully skips this section with a try/except block and prints a message.\n",
    "- Interpretation note: In medical contexts, **minimizing False Negatives (FN)** is typically critical â€” missing a disease diagnosis is more harmful than a false alarm.\n",
    "\n",
    "**Model Performance Interpretation**\n",
    "- **High Accuracy + High F1:** Good overall balance and performance.\n",
    "- **High Recall, Low Precision:** Model detects most diseases but has many false alarms.\n",
    "- **High Precision, Low Recall:** Model is conservative; few false alarms but misses some disease cases.\n",
    "- **ROC-AUC near 1.0:** Excellent discrimination; near 0.5: random classifier.\n",
    "\n",
    "**Next steps**\n",
    "- Review the evaluation metrics and visualizations to understand model strengths/weaknesses.\n",
    "- If performance is unsatisfactory, consider:\n",
    "  - Adjusting hyperparameters further (Phase 6).\n",
    "  - Collecting more data or engineering additional features (Phase 4).\n",
    "  - Trying alternative models (Phase 5).\n",
    "- For deployment, consider the trade-off between precision and recall based on clinical requirements.\n",
    "\n",
    "**Quick validation**\n",
    "After running, confirm these files exist:\n",
    "- `visualizations/14_evaluation.png` âœ“\n",
    "- `visualizations/15_feature_importance.png` âœ“ (if model supports it)\n",
    "- Console output shows all 5 metrics and classification report âœ“\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
